# Syllabus, Textbooks & References  
## Causality and Machine Learning (ML808)

**Course syllabus:** Spring 2026  
**Course time & location:** Tuesdays & Thursdays 3:00 – 4:20 PM, LH1  
**Instructor:** Kun Zhang & Jin Tian  

---

## Description

In the past decades, significant progress has been made in tackling long-standing causality problems, such as discovering causality from observational data and inferring causal effects. Moreover, it has recently been shown that the causal perspective aids in understanding and solving various machine learning problems such as transfer learning, out-of-distribution prediction, disentanglement, representation learning, and adversarial vulnerability. Accordingly, this course is concerned with understanding causality, learning it from observational data, and using it to tackle other learning problems.

The course covers representations of causal models, how causality is different from association, methods for causal discovery and causal representation learning, and how causality enhances advanced learning tasks, including generative AI. We will address the following questions:

- Why is causality essential?  
- How can we learn it, including latent variables, from observational data?  
- How can we make sure the estimated representation is causal?  
- What role does causality play in learning under data heterogeneity?  
- Can causal principles make generative AI more controllable and capable of extrapolation?  
- How can deep learning benefit from a causal perspective?  

Two main causality problems are emphasized:

1. **Causal discovery / causal representation learning.** It is well known that “correlation does not imply causality,” but we will make it more precise by asking what assumptions, what information in the data, and what procedures enable us to successfully recover causal information. Causal relations may happen among the underlying hidden variables—we will also see how to uncover the underlying hidden “causal” variables as well as their causal relations from the measured variables.  
2. **How to properly make use of causal information.** This includes identification of causal effects, counterfactual reasoning, and improving machine learning with causal knowledge.

---

## Course objectives

As an outcome of this course, participants are expected to:

- Understand how causality is different from association and why it is useful  
- Get familiar with graphical models, causality-related concepts and principles, and emerging approaches to causal discovery or causal representation learning from observational data  
- Be acquainted with the state-of-the-art of causality research in different disciplines  
- Be able to develop suitable methods for causal representation learning or causal discovery and causal inference to address problems in specific domains  
- Properly leverage causality in understanding and solving advanced machine learning and artificial intelligence problems  
- Identify and formulate causal problems in your respective fields, and be able to find potential solutions  

---

## Who can attend

Prerequisites are not required, but introductory statistics or machine learning would be helpful. This course is accessible to students from across disciplines—we especially welcome students from different departments.

---

## Course materials

Reading materials will be available online or distributed in class. In addition, we will refer to several chapters of the following books frequently (some chapters will be available online):

- Peter Spirtes, Clark Glymour, and Richard Scheines (SGS). **Causation, Prediction, and Search**, 2nd edition. MIT Press, 2000.  
- Judea Pearl. **Causality: Models, Reasoning and Inference**, 2nd edition. Cambridge University Press, 2009.  
- Elias Bareinboim. **Causal Artificial Intelligence: A Roadmap for Building Causally Intelligent Systems.** https://causalai-book.net/  

---

## Grading

- **Class participation:** 5%  
  - You are allowed to miss **one** class without any penalty; after that, missing each class lowers your final course grade by **1%** (5% is the maximum), unless you get an approval.  
- **Active involvement in in-class discussions:** 5%  
  - Raising or answering questions and participation in discussions.  

**Assignments & exam**
- **Three homework assignments + midterm exam**  
  - Homework assignments: **20%**  
    - Submit on **Moodle** as MS Word or PDF (in special situations, you may submit by email to the course instructor).  
    - **20%** deducted if late unless instructor approval is obtained in advance.  
  - Midterm exam: **30%**  

**Project**
- **Project/essay proposal (one page):** 5% — due **6 March, 11:59 PM**  
- **Final project report:** 25% — due **30 April, 11:59 PM**  
- **Final presentation:** 10% — time to be announced  
- Please work together with the instructor to decide on the topic for your project/essay by **3 March**.  
- You are encouraged to use the template available in **NeurIPS 2025 LaTeX style file** for the proposal and report (not mandatory).

---

## Class schedule (subject to change)

We will answer the following questions throughout the course:

- What is causality? How can we define it?  
- Why do humans care about causality? How can AI benefit from it?  
- Can AI learn causal relations (e.g., which mutations cause the disease)? Can AI recover hidden causal variables? How can this procedure benefit automated scientific discovery?  
- How can causality benefit generative AI? Can AI learn concepts and use them to achieve controllability & extrapolation? How can LLMs perform (causal and logical) reasoning in a principled way?  

The course is divided into nine sessions; see below.

---

# Part I. Introduction (1 week)

## 01/13 (Tue) & 01/15 (Thu): Introduction — Concepts, problems, and a big picture of machine learning
- What is causality and why causality  
- Simpson’s paradox  
- Introduction to machine learning and artificial intelligence & how they are connected with causality  
- Causality-related concepts, principles, and problems: definition of causality, motivation for causal analysis, directed acyclic graphs, interventions, structural equation models  
- ★Discussion: Why do we care about causality?  
- Research problems in causality: Causal discovery, causality for machine learning, identification of causal effects, counterfactual reasoning, generative AI, (automated) scientific discovery  
- Causal generative AI  
- Summary of fundamental problems and recent achievements  
- ✓Reading:  
  1. Pages 1–6 of “Causal discovery and inference: concepts and recent methodological advances” (Spirtes & Zhang), *Applied Informatics*, 2016  
  2. Chapter 1 of Pearl’s book  
- ★Open discussion: What do you think of causal analysis in your field?

**Lecture materials (Week 1)**
- Introduction part 1: broad picture of causality — https://lms.mbzuai.ac.ae/mod/resource/view.php?id=29873  
- Introduction part 2: Typical causality problem and initial ideas — https://lms.mbzuai.ac.ae/mod/resource/view.php?id=29874  

---

# Part II. Preliminaries (2.5 weeks)  
*Probabilistic thinking and intuitive causal thinking to basic machine learning, graphical models, and traditional multivariate analysis*

## 01/20 (Tue): From probability theory to statistics
- We will begin by answering the question, “Will the sun rise tomorrow?”, with convincing arguments  
- Probability axioms, discrete and continuous variables  
- Statistical independence and conditional independence  
- Sample statistics: expectation, covariance, and correlation; uncorrelatedness vs. independence  
- Central Limit Theorem and Cramer Decomposition Theorem  
- Gaussian distribution  
  - Why is it widely assumed but rarely encountered?  
  - Is it a blessing or a challenge to causal discovery?  
- Three ways of making use of data  
- Bayes’ rule  
- Statistical tests  
- Maximum likelihood estimation (point estimation)  
- Linear regression  
- ✓Reading: Chapter on maximum likelihood estimation of *Probability and Statistical Inference* (R. V. Hogg, E. A. Tanis, D. L. Zimmerman)

**Lecture materials (Week 2)**
- From probability theory to statistics — https://lms.mbzuai.ac.ae/mod/resource/view.php?id=30204  

## 01/22 (Thu): Traditional supervised machine learning — settings, assumptions, basic methods, and model selection
- Supervised learning  
- From linear to nonlinear models  
- Nonparametric models  
- Bias-variance tradeoff  
- Model selection  

## 01/27 (Tue): Unsupervised learning & reinforcement learning
- Unsupervised learning  
- Two ways to “simplify” data  
- Assumptions underlying clustering  
- Introduction to reinforcement learning  

## 01/29 (Thu): Intuitive causal thinking — graphical models, d-separation, and representation of causal relations
- Graphical models  
- d-separation  
- Markov conditions  
- Causal graphical models  

## 02/03 (Tue): Multivariate analysis — goals, techniques, and connections to causal discovery
- Problem of Principal Component Analysis (PCA)  
- Development of PCA: maximum variance vs. least reconstruction error  
- Eigenvalue decomposition  
- Properties of PCA and relation to regression  
- Factor analysis: model assumptions and identifiability  
- Independent component analysis (ICA): linear and nonlinear cases  
- The (imprecise) connection between multivariate analysis methods with causal analysis  
- ➡Assignment 1 released  

**Lecture materials (Week 2)**
- Introduction part 2: Typical causal problems — https://lms.mbzuai.ac.ae/mod/resource/view.php?id=30203  

---

# Part III. Structural causal models & identification of causal effects (1 week)

## 02/05 (Thu): Structural causal models
- Pearl's Causal Hierarchy  
- Interventions and causal effects  
- Potential outcome framework vs. structural causal models  
- ✓Reading: Chapter 2 of Bareinboim’s book  

## 02/10 (Tue): Identifiability & identification of causal effects
- Identifiability of causal effects  
- Controlling confounding bias: back-door and front-door criteria  
- The Do-calculus  
- Algorithmic approach for identification  
- Causal effect estimation  
- ✓Reading: Chapter 4 of Bareinboim’s book  

---

# Part IV. Traditional approaches to causal discovery (0.5 week)  
*“Independence” in causal models, constraint- and score-based causal discovery*

## 02/12 (Thu): “Independence” in causal models & constraint-based causal discovery
- “Independence” implied by causal models: general ideas  
- “Independence” instantiation 1: conditional independence for causal discovery  
- PC algorithm for causal discovery  
- FCI algorithm for causal discovery  
- GES algorithm for causal discovery  
- Demonstration: Using causal-learn or TETRA for causal analysis  
- ✓Reading:  
  1. Chapters 5.4.1 & 5.4.2 of the SGS book  
  2. Chapter 6.7 of the SGS book  
  3. “Optimal Structure Identification With Greedy Search” (Chickering), *JMLR*, 2002  

---

# Part V. Functional causal model-based approaches (2 weeks)  
*Linear, non-Gaussian methods and beyond*

## 02/17 (Tue): Linear, non-Gaussian, acyclic causal models (LiNGAM)
- Structural equation models and independence noise (“Independence” instantiation 2)  
- LiNGAM: identifiability and identification of the causal model  
- ✓Reading: Shimizu et al., “A linear non-Gaussian acyclic model for causal discovery,” *JMLR*, 2006  

## 02/19 (Thu): Estimating LiNGAM + estimation of cyclic causal models (feedback)
- ICA and its relation to LiNGAM  
- Estimating LiNGAM with independent noise condition  
- Interpretation of feedback in causal representations  
- Linear causal discovery in the presence of feedback  
- ICA-based local causal discovery  
- ✓Reading: Lacerda et al., “Discovering cyclic causal models by independent components analysis,” *UAI*, 2008  

## 02/24 (Tue): Traditional causal discovery in the presence of confounders
- Linear causal discovery in the presence of confounders: what is identifiable and how to estimate it?  
- ✓Reading:  
  1. Hoyer et al., “Estimation of causal effects using linear nonGaussian causal models with hidden variables,” *International Journal of Approximate Reasoning*, 2008  
  2. Salehkaleybar et al., “Learning Linear Non-Gaussian Causal Models in the Presence of Latent Variables,” *JMLR*, 2020  
- ★Discussion: What do you think of causal discovery and those methods?  

## 02/26 (Thu): Nonlinear causal models & Independent causal mechanism for causal discovery
- Post-nonlinear causal models  
- Nonlinear additive noise models  
- Estimation of nonlinear causal models  
- “Independent nonlinear mechanism” for causal discovery in deterministic cases  
- “Independent mechanism” in linear, high-dimensional case  
- ✓Reading: Pages 11–18 of “Causal discovery and inference: concepts and recent methodological advances” (Spirtes & Zhang), *Applied Informatics*, 2016  
- ➡Assignment 2 released  

---

# Part VI. Practical issues in causal discovery (1 week)

## 03/03 (Tue): Causal discovery with nonstationarity, mixed variable types, and selection bias
- Modeling causal processes with continuous/discrete variables  
- Causal discovery and visualization of nonstationary causal models  
- Effects of different types of selection bias  
- Causal discovery in the presence of selection bias  
- ✓Reading:  
  1. Huang et al., “Causal discovery from heterogeneous/nonstationary data,” *JMLR*, 2020  
  2. Zhang et al., “On the Identifiability and Estimation of Functional Causal Models in the Presence of Outcome-Dependent Selection,” *UAI*, 2016  
  3. Zheng et al., “Detecting and Identifying Selection Structure in Sequential Data,” *ICML*, 2024  

## 03/05 (Thu): Measurement error, missing values, and time series
- Causal discovery in the presence of measurement error  
- Missing data as a causal problem & causal discovery in the presence of missing data  
- Granger causality and its relation to constraint-based causal discovery  
- Structural equal models for causal discovery from time series: Granger causality with instantaneous effects, causal discovery from subsampled data, causal discovery from partially observed processes  
- ✓Reading:  
  1. Zhang et al., “Causal discovery in the presence of measurement error: Identifiability conditions,” *UAI*, 2018  
  2. Tu et al., “Causal discovery in the presence of missing data,” *AISTATS*, 2019  
  3. Granger, “Testing for causality: a personal viewpoint,” *J. Econ. Dyn. Control*, 1980  

---

# Part VII. Causal representation learning (CRL) (1.5 weeks)

## 03/10 (Tue): CRL in the IID case — benefits from functional constraints or sparsity
- Estimation of latent variables and their causal relations  
- Tetrad conditions  
- Rank deficiency and Rank-based Latent Causal Discovery (RLCD)  
- Generalized Independent Noise (GIN) conditions  
- Sparsity constraints for identifiability of nonlinear ICA  
- ✓Reading:  
  1. Dong et al., “A Versatile Causal Discovery Framework to Allow Causally-Related Hidden Variables,” *ICLR*, 2024  
  2. Xie et al., “Generalized Independent Noise Condition for Estimating Linear Non-Gaussian Latent Variable Causal Graphs,” *NeurIPS*, 2020  
  3. Zheng & Zhang, “Generalizing Nonlinear ICA Beyond Structural Sparsity,” *NeurIPS*, 2023  

## 03/12 (Thu): CRL from changes + CRL from temporal data
- Nonlinear ICA with surrogate variables  
- Partial disentanglement with identifiable changing components  
- General, nonparametric case  
- Minimal change principle  
- Why temporal information helps  
- Temporal disentanglement  
- With instantaneous relations  
- ✓Reading:  
  1. Hyvärinen et al., “Nonlinear ICA using auxiliary variables and generalized contrastive learning,” *AISTATS*, 2019  
  2. Kong et al., “Partial disentanglement for domain adaptation,” *ICML*, 2022  
  3. Zhang et al., “Causal Representation Learning from Multiple Distributions: A General Setting,” *ICML*, 2024  
  4. Yao et al., “Temporally Disentangled Representation Learning,” *NeurIPS*, 2022  

## 03/17 & 03/19: No class

## 03/24 (Tue): Real problems of CRL
- Psychometric studies  
- Multi-model causal discovery with latent variables  
- Refined (causal) CLIP model  
- ✓Reading:  
  1. Xie et al., “Multi-domain image generation and translation with identifiability guarantees,” *ICLR*, 2023  
  2. A paper on refined causal CLIP model will be shared later  

---

# Part VIII. Counterfactual reasoning (1.5 weeks)

## 03/26 (Thu): Counterfactual reasoning
- Counterfactual reasoning vs. traditional prediction  
- Methods for counterfactual reasoning  
- Partial identification of counterfactuals  
- ✓Reading: Chapter 5 of Bareinboim’s book  

## 03/31 (Tue): Counterfactual quantities
- Effect of the treatment on the treated  
- Probability of causation  
- Direct & indirect effects  
- Path-specific effects  
- ✓Reading: Chapter 5 of Bareinboim’s book  

## 04/02 (Thu): Causal fairness
- Fairness challenges in AI  
- Causal fairness analysis  
- Structural fairness measures  
- ✓Reading: Chapter 6 of Bareinboim’s book  

---

# Part IX. Causal view for machine learning and artificial intelligence (1.5 weeks)

## 04/07 (Tue): Semi-supervised learning, reinforcement learning, and large models
- Semi-supervised learning  
- Reinforcement learning: learning and using causal representations  
- Causality of/for large models  
- ✓Reading:  
  1. Schölkopf et al., “On causal and anticausal learning,” *ICML*, 2012  
  2. Huang et al., “AdaRL: What, Where, and How to Adapt in Transfer Reinforcement Learning,” *ICLR*, 2022  
  3. A paper on causality of/for large models will be provided later (rapidly evolving field)  

## 04/09 (Thu): Transfer learning, image translation, and causal generative AI
- A picture of machine learning, especially deep learning  
- Transfer learning  
- Image-to-image translation: a causal perspective  
- ✓Reading:  
  1. Zhang et al., “Domain Adaptation as a Problem of Inference on Graphical Models,” *NeurIPS*, 2020  
  2. Xie et al., “Multi-domain image generation and translation with identifiability guarantees,” *ICLR*, 2023  
- ★Discussion: How is general-purpose AI connected to causality and how to achieve it?  

## 04/14 (Tue): Unsupervised deep learning, deep generative models, and causal generative AI
- Adversarial vulnerability  
- How causality helps in unsupervised deep learning  
- Disentanglement  
- Autoencoder, GANs, and Stable diffusion models  
- How causal learning benefits generative AI  
- Learning concepts from image-text pairs and using them for controllable image generation and editing  
- Issues with autoregressive text generation & how to improve it  
- Discussing next steps  
- ✓Reading:  
  1. Goodfellow et al., “Generative Adversarial Nets,” *NIPS*, 2014  
  2. Xie et al., “Learning Vision and Language Concepts for Controllable Image Generation,” *ICML*, 2025  
  3. Tang et al., “Reflection-Window Decoding: Text Generation with Selective Refinement,” *ICML*, 2025  
  4. Xie et al., “Causal Compositional Image Generation with Minimal Change,” *arXiv*, 2024  
- ➡Assignment 3 released  

## 04/16: Research Showcase — no class

---

# Part X. Real applications, review, and outlook (1 week)

## 04/21 (Tue): Applications of causal inference, causal discovery, and causal representation learning
- Causal reinforcement learning  
- Causal analysis in neuroscience (especially brain network discovery from fMRI)  
- Causal analysis in finance and biology  
- Real data sets for causal discovery  
- Discussion: causality vs. selection  

## 04/23 (Thu): Review and Outlook
- Review: Why causality? How do we find & make use of causality?  
- How to achieve automated scientific discovery (generation of causal hypotheses followed by verifications)?  
- Discussion:  
  - Can we avoid using “causality”?  
  - Causality in the era of large models?  
  - Causality facilitates the 2nd scientific revolution?  

---

# Final Project: Topics, Requirements, and Key Dates

Participants are encouraged to present your own causality-related problems or data sets for the final project. Alternatively, you can choose one from the following topics (reading materials will be provided to you):

- Causal generative AI for image generation, video generation, or text generation  
- Domain specific causal discovery (e.g., for fMRI, MEG, economic data, financial data, and climate data)  
- Causal perspective of multi-task learning  
- Causal treatment of out-of-distribution prediction  
- Prediction in nonstationary environments: invariant representations or the ability to adapt?  
- Understanding “feedback” in directed graphical causal representations  
- Causality and heterogeneity in learning  
- Causality and transfer learning with different feature spaces: why and how does causal knowledge help?  
- Causal discovery and complexity measures  
- Causal discovery in the presence of confounders from distribution changes  
- Towards “universal” causal discovery  
- Creativity by causal knowledge integration and counterfactual reasoning  
- Causal links vs. associations between genes, traits, and disease (c.f. “Integrative analysis of 111 reference human epigenomes”)  
- Causal analysis in stock market and interpretation of the causal relations  
- Causality in climate analysis (e.g., prediction and understanding of El Niño; c.f. the “Azimuth El Niño Project”)  
- Causality and prediction in nonstationary environments (e.g., the “parable of Google Flu Trends”)  
- Finding causal knowledge and using it for crime control  
- Causality-based computational social science  
- Large language models and causality  
- Extrapolation  

Students are expected to present the selected problems, make progress on the topics, and summarize their achievements. You may complete them alone or in two-person groups.

## Timetable for the final project

- **03/03:** Last day to decide on the topic of your project/essay  
- **03/06 (11:59 PM):** Submit a short (one-page) description of the proposed final project & initial ideas  
  - We recommend using the NeurIPS 2025 LaTeX style template  
- **04/30 (11:59 PM):** Final version of the project report due  
  - Please use the NeurIPS 2025 LaTeX style template  

---

## Recommended Textbooks

- Peter Spirtes, Clark Glymour, and Richard Scheines (SGS). **Causation, Prediction, and Search**, 2nd edition. MIT Press, 2000.  
- Judea Pearl. **Causality: Models, Reasoning and Inference**, 2nd edition. Cambridge University Press, 2009.  
- Elias Bareinboim. **Causal Artificial Intelligence: A Roadmap for Building Causally Intelligent Systems.** https://causalai-book.net/  

---

## Weekly materials (in progress)

- **Week 1**
  - Introduction part 1: broad picture of causality — https://lms.mbzuai.ac.ae/mod/resource/view.php?id=29873  
  - Introduction part 2: Typical causality problem and initial ideas — https://lms.mbzuai.ac.ae/mod/resource/view.php?id=29874  
- **Week 2**
  - Introduction part 2: Typical causal problems — https://lms.mbzuai.ac.ae/mod/resource/view.php?id=30203  
  - From probability theory to statistics — https://lms.mbzuai.ac.ae/mod/resource/view.php?id=30204  
- **Week 3**
  - *Coming soon*
